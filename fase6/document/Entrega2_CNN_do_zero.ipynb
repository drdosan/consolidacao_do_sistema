{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drdosan/cap1-despertar-da-rede-neural/blob/main/document/Entrega2_CNN_do_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-230714b1"
      },
      "source": [
        "# üìò Entrega 2 ‚Äî CNN do zero (Classifica√ß√£o: *blusas* vs *sapatos*)\n",
        "\n",
        "Este notebook implementa uma **CNN simples do zero** (PyTorch) para classificar imagens em duas classes: **blusas** e **sapatos**.\n",
        "\n",
        "Ele foi pensado para complementar a compara√ß√£o com:\n",
        "- **YOLO Otimizado** (Entrega 1)\n",
        "- **YOLO Tradicional** (defaults: `--img 640`, sem `--hyp` custom)\n",
        "\n",
        "üîé **Objetivo pedag√≥gico**: classificador **n√£o** faz detec√ß√£o (sem *bounding boxes*). O foco √© **acur√°cia por classe** e **tempo de infer√™ncia**.\n"
      ],
      "id": "md-230714b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-a183313b"
      },
      "source": [
        "## 1) Imports e Configura√ß√µes\n",
        "Nesta se√ß√£o definimos caminhos, *seeds* e dispositivo (CPU/GPU). Ajuste `BASE` se necess√°rio.\n"
      ],
      "id": "md-a183313b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-7ab412b3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, time, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/roupas\"\n",
        "TRAIN_DIR = f\"{BASE}/images/train\"\n",
        "VAL_DIR   = f\"{BASE}/images/val\"\n",
        "TEST_DIR  = f\"{BASE}/images/test\"\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "id": "cd-7ab412b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-0f4dd083"
      },
      "source": [
        "## 2) Data Loaders\n",
        "Usamos **ImageFolder** com pastas por classe e normaliza√ß√£o padr√£o do ImageNet. No treino, aplicamos *augmentations* leves.\n",
        "\n",
        "Estrutura esperada:\n",
        "```\n",
        "/images/train/blusas, /images/train/sapatos\n",
        "/images/val/blusas,   /images/val/sapatos\n",
        "/images/test/blusas,  /images/test/sapatos\n",
        "```\n"
      ],
      "id": "md-0f4dd083"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-24c5a8a0"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "img_size = 224\n",
        "\n",
        "tf_train = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "tf_eval = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(TRAIN_DIR, transform=tf_train)\n",
        "val_ds   = datasets.ImageFolder(VAL_DIR,   transform=tf_eval)\n",
        "test_ds  = datasets.ImageFolder(TEST_DIR,  transform=tf_eval)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "len(train_ds), len(val_ds), len(test_ds), class_names"
      ],
      "id": "cd-24c5a8a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-8d688239"
      },
      "source": [
        "## 3) Modelo ‚Äî TinyCNN (simples e did√°tico)\n",
        "Arquitetura pequena para **convergir r√°pido** nesse dataset reduzido: 4 conv blocks + *global average pooling* + MLP.\n"
      ],
      "id": "md-8d688239"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-70b10c52"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = TinyCNN(num_classes=2).to(device)\n",
        "sum(p.numel() for p in model.parameters())/1e6, device"
      ],
      "id": "cd-70b10c52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-ebc9c2a7"
      },
      "source": [
        "## 4) Treino + Valida√ß√£o\n",
        "Treinamos por **20 √©pocas** (equil√≠brio entre tempo e risco de *overfitting* num dataset pequeno). Usamos `ReduceLROnPlateau` para reduzir a taxa de aprendizado se o *val_loss* estagnar. Salvamos o **melhor estado** com base na *val_acc*.\n"
      ],
      "id": "md-ebc9c2a7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-f794cfca"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "epochs = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train() if train else model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.set_grad_enabled(train):\n",
        "        for imgs, labels in loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            if train:\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            loss_sum += loss.item()*imgs.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "history = {\"tr_loss\": [], \"tr_acc\": [], \"va_loss\": [], \"va_acc\": []}\n",
        "best_val_acc, best_state = 0.0, None\n",
        "\n",
        "for ep in range(1, epochs+1):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
        "    scheduler.step(va_loss)\n",
        "    history[\"tr_loss\"].append(tr_loss); history[\"tr_acc\"].append(tr_acc)\n",
        "    history[\"va_loss\"].append(va_loss); history[\"va_acc\"].append(va_acc)\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc, best_state = va_acc, {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "    print(f\"Epoch {ep:02d} | train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={va_loss:.4f} acc={va_acc:.3f}\")\n",
        "\n",
        "if best_state:\n",
        "    model.load_state_dict(best_state)\n",
        "save_path = f\"{BASE}/runs/cnn_from_scratch.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Modelo salvo em:\", save_path)"
      ],
      "id": "cd-f794cfca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-e9280086"
      },
      "source": [
        "## 5) (Opcional) Curvas de Treino\n",
        "Gr√°ficos de *loss* e *accuracy* para visualizar converg√™ncia e poss√≠vel *overfitting*.\n"
      ],
      "id": "md-e9280086"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-aee712f6"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(); plt.plot(history['tr_loss']); plt.plot(history['va_loss']); plt.title('Loss'); plt.legend(['train','val']); plt.xlabel('epoch'); plt.ylabel('loss'); plt.show()\n",
        "plt.figure(); plt.plot(history['tr_acc']);  plt.plot(history['va_acc']);  plt.title('Accuracy'); plt.legend(['train','val']); plt.xlabel('epoch'); plt.ylabel('acc');  plt.show()"
      ],
      "id": "cd-aee712f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-9f859a9d"
      },
      "source": [
        "## 6) Avalia√ß√£o no Teste\n",
        "Geramos **tempo m√©dio de infer√™ncia (s/img)**, **classification_report** (precision/recall/F1 por classe) e **matriz de confus√£o**.\n"
      ],
      "id": "md-9f859a9d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-ce8ae529"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "t0 = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        logits = model(imgs)\n",
        "        preds = logits.argmax(1).cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(labels.numpy())\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "import numpy as np\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "print(f\"‚è±Ô∏è Tempo m√©dio de infer√™ncia (aprox.): {(t1 - t0)/len(test_ds):.4f} s/img\")\n",
        "print(\"\\n== Classification Report ==\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\n== Matriz de Confus√£o ==\\n\", cm)"
      ],
      "id": "cd-ce8ae529"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-56acb4b4"
      },
      "source": [
        "## 7) Carregar e Usar o Modelo Salvo (opcional)\n",
        "Exemplo de como restaurar o estado salvo e fazer infer√™ncia posteriormente.\n"
      ],
      "id": "md-56acb4b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-bb198a94"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "loaded = TinyCNN(num_classes=2).to(device)\n",
        "loaded.load_state_dict(torch.load(f\"{BASE}/runs/cnn_from_scratch.pt\", map_location=device))\n",
        "loaded.eval()\n",
        "loaded"
      ],
      "id": "cd-bb198a94"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-8ece0879"
      },
      "source": [
        "## 8) Como Reportar no Relat√≥rio da Entrega 2\n",
        "- **Acur√°cia global** e **F1 por classe** (tabela do *classification_report*).\n",
        "- **Matriz de confus√£o** (erros por classe).\n",
        "- **Tempo m√©dio de infer√™ncia** (s/img) desta CNN.\n",
        "- **Comparar com YOLO** (mAP/Precis√£o/Recall e tempo de infer√™ncia do seu `val.py`/`detect.py`).\n",
        "\n",
        "üß≠ **Mensagem final**: a CNN √© simples, r√°pida e funciona bem quando a imagem tem **um objeto dominante** e n√£o √© necess√°rio localizar/contar. J√° o YOLO √© prefer√≠vel quando voc√™ precisa **detectar e localizar** objetos com *bounding boxes* (invent√°rio, seguran√ßa, etc.).\n"
      ],
      "id": "md-8ece0879"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}